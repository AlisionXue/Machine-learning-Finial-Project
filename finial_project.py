# -*- coding: utf-8 -*-
"""Finial_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/AlisionXue/Machine-learning-Finial-Project/blob/main/Finial_Project.ipynb
"""

# -*- coding: utf-8 -*-
"""Diabetes Readmission Prediction Model """
# Authors: Junchang Yang, Chang Xue, Pardhu Mattupalli

# ========================== 1. Environment Setup ==========================
# Install dependencies (Kaggle API already included)
!pip install -q pandas numpy scikit-learn xgboost imbalanced-learn matplotlib kaggle

# ========================== 2. Auto Configure Kaggle API ==========================
import os
# Replace with your Kaggle username and API Key
os.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'  # Important: Need to modify!
os.environ['KAGGLE_KEY'] = 'your_kaggle_api_key'        # Important: Need to modify!

# Auto create config file
!mkdir -p ~/.kaggle
!echo '{"username":"'$KAGGLE_USERNAME'","key":"'$KAGGLE_KEY'"}' > ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# ========================== 3. Dataset Download ==========================
print("‚è≥ Downloading dataset...")
!kaggle datasets download -d brandao/diabetes
!unzip -q diabetes.zip
print("‚úÖ Dataset download completed!")

# ========================== 4. Data Loading & Validation ==========================
import pandas as pd

try:
    df = pd.read_csv('diabetic_data.csv')  # Note actual filename
    print("\n=== Data loaded successfully ===")
    print("üìä Data shape:", df.shape)
    print("üîç First 5 rows preview:")
    print(df.head())
except Exception as e:
    print("‚ùå Data loading failed:", e)
    !ls -l  # Show current file list
    raise

# ========================== 5. Data Preprocessing ==========================
print("\n=== Data Preprocessing ===")

# Target variable transformation
df['readmitted'] = df['readmitted'].map({'<30': 1, '>30': 0, 'NO': 0})
print("üéØ Target variable distribution:\n", df['readmitted'].value_counts())

# Handle missing values (original data uses '?' for missing)
df = df.replace('?', pd.NA)
df = df.fillna(-1)

# ========================== 6. Feature Engineering ==========================
print("\n=== Feature Engineering ===")

# Verify actual column names
print("üìù Dataset columns:", df.columns.tolist())

# Adjusted feature list (modified based on actual columns)
features = [
    'time_in_hospital',
    'num_lab_procedures',
    'num_procedures',
    'num_medications',
    'number_outpatient',
    'number_emergency',
    'number_inpatient',
    'number_diagnoses',
    'age',
    'insulin',
    'diabetesMed',
    'change',
    'gender',
    'A1Cresult'
]

# Check feature existence
missing_features = [f for f in features if f not in df.columns]
if missing_features:
    print(f"‚ùå Missing feature columns: {missing_features}")
    raise KeyError("Feature columns mismatch")
else:
    print("‚úÖ All feature columns exist")

X = df[features]
y = df['readmitted']

# ‚Äî‚Äî Convert all categorical features to string to avoid int/str mixed type errors ‚Äî‚Äî
categorical_features = [
    'age',
    'insulin',
    'diabetesMed',
    'change',
    'gender',
    'A1Cresult'
]
X[categorical_features] = X[categorical_features].astype(str)

# ========================== 7. Data Preprocessing Pipeline ==========================
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline

# Numeric feature standardization
numeric_features = [
    'time_in_hospital',
    'num_lab_procedures',
    'num_medications'
]

# Categorical feature encoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# ========================== 8. Model Configuration ==========================
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split

# Define model collection
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "XGBoost": XGBClassifier(
        scale_pos_weight=sum(y == 0) / sum(y == 1),  # Handle class imbalance
        eval_metric='logloss',
        use_label_encoder=False
    )
}

# ========================== 9. Training & Evaluation ==========================
print("\n=== Model Training ===")
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    ConfusionMatrixDisplay,
    RocCurveDisplay
)
import matplotlib.pyplot as plt

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

results = {}

for name, model in models.items():
    print(f"\nüöÄ Training {name}...")

    # Build pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('smote', SMOTE(random_state=42)),  # Oversampling
        ('classifier', model)
    ])

    # Train model
    pipeline.fit(X_train, y_train)

    # Predict
    y_pred = pipeline.predict(X_test)

    # Calculate metrics
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1': f1_score(y_test, y_pred),
        'ROC AUC': roc_auc_score(y_test, y_pred)
    }
    results[name] = metrics

    # Visualization
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax[0], cmap='Blues')
    RocCurveDisplay.from_estimator(pipeline, X_test, y_test, ax=ax[1])
    plt.suptitle(f"{name} Performance", y=1.02)
    plt.show()

# ========================== 10. Results Display ==========================
print("\n=== Final Results ===")
results_df = pd.DataFrame(results).T
print(results_df.sort_values(by='Recall', ascending=False))

# ========================== Deployment Recommendations ==========================
print("\n‚≠ê Deployment Recommendations:")
best_model = results_df['Recall'].idxmax()
print(f"- Recommended model: {best_model} (highest recall)")
print("- Deployment approach:")
print("  1. Save trained pipeline as .pkl file")
print("  2. Integrate into hospital EMR system")
print("  3. Automatically generate risk score when new patients are discharged")
print("  4. High-risk patients automatically trigger follow-up by care team")